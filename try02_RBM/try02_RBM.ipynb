{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from RBM import RBM\n",
    "#from load_dataset import MNIST\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "\tmodel = torch.nn.Sequential(\n",
    "\t\ttorch.nn.Linear(20, 1600),\n",
    "\t\ttorch.nn.Sigmoid(),\n",
    "\t\ttorch.nn.Linear(1600, 8),\n",
    "\t\ttorch.nn.Softmax(dim=1),\n",
    "\t)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(x, y, batch_size=64):\n",
    "\tx = x[:int(x.shape[0] - x.shape[0]%batch_size)]\n",
    "\tx = torch.reshape(x, (x.shape[0]//batch_size, batch_size, x.shape[1]))\n",
    "\ty = y[:int(y.shape[0] - y.shape[0]%batch_size)]\n",
    "\ty = torch.reshape(y, (y.shape[0]//batch_size, batch_size))\n",
    "\treturn {'x':x, 'y':y}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, train_x, train_y, test_x, test_y, epoch):\n",
    "\tcriterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\toutput_test = model(test_x)\n",
    "\tloss_test = criterion(output_test, test_y).item()\n",
    "\toutput_test = torch.argmax(output_test, axis=1)\n",
    "\tacc_test = torch.sum(output_test == test_y).item()/test_y.shape[0]\n",
    "\n",
    "\toutput_train = model(train_x)\n",
    "\tloss_train = criterion(output_train, train_y).item()\n",
    "\toutput_train = torch.argmax(output_train, axis=1)\n",
    "\tacc_train = torch.sum(output_train == train_y).item()/train_y.shape[0]\n",
    "\n",
    "\treturn epoch, loss_test, loss_train, acc_test, acc_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, train_x, train_y, test_x, test_y, epochs=200):\n",
    "\tdataset = generate_batches(x, y)\n",
    "\n",
    "\tcriterion = torch.nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\ttraining = trange(epochs)\n",
    "\tprogress = []\n",
    "\tfor epoch in training:\n",
    "\t\trunning_loss = 0\n",
    "\t\tacc = 0\n",
    "\t\tfor batch_x, target in zip(dataset['x'], dataset['y']):\n",
    "\t\t\toutput = model(batch_x)\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\t\toutput = torch.argmax(output, dim=1)\n",
    "\t\t\tacc += torch.sum(output == target).item()/target.shape[0]\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\trunning_loss /= len(dataset['y'])\n",
    "\t\tacc /= len(dataset['y'])\n",
    "\t\tprogress.append(test(model, train_x, train_y, test_x, test_y, epoch+1))\n",
    "\t\ttraining.set_description(str({'epoch': epoch+1, 'loss': round(running_loss, 4), 'acc': round(acc, 4)}))\n",
    "\n",
    "\treturn model, progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "#from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as tr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:/Users/kbj/Downloads/data/train/x/'\n",
    "data_train_x_list = os.listdir('C:/Users/kbj/Downloads/data/train/x/')\n",
    "path = 'C:/Users/kbj/Downloads/data/train/x/'\n",
    "x_train_mean = np.array([])\n",
    "x_train_var = np.array([])\n",
    "x_train_std = np.array([])\n",
    "x_train_max = np.array([])\n",
    "x_train_min = np.array([])\n",
    "\n",
    "for i in data_train_x_list:\n",
    "    df = pd.read_csv(path+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    x_train_mean = np.append(x_train_mean, mean)\n",
    "    x_train_var = np.append(x_train_var, var)\n",
    "    x_train_std = np.append(x_train_std, std)\n",
    "    x_train_max = np.append(x_train_max, max)\n",
    "    x_train_min = np.append(x_train_min, min)\n",
    "\n",
    "x_train_feature = np.concatenate((x_train_mean.reshape(-1,1), x_train_var.reshape(-1,1), x_train_std.reshape(-1,1), x_train_max.reshape(-1,1), x_train_min.reshape(-1,1)), axis=1)\n",
    "x_train_feature.shape\n",
    "\n",
    "path = 'C:/Users/kbj/Downloads/data/train/y/'\n",
    "data_train_y_list = os.listdir('C:/Users/kbj/Downloads/data/train/y/')\n",
    "path = 'C:/Users/kbj/Downloads/data/train/y/'\n",
    "y_train_mean = np.array([])\n",
    "y_train_var = np.array([])\n",
    "y_train_std = np.array([])\n",
    "y_train_max = np.array([])\n",
    "y_train_min = np.array([])\n",
    "\n",
    "for i in data_train_y_list:\n",
    "    df = pd.read_csv(path+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    y_train_mean = np.append(y_train_mean, mean)\n",
    "    y_train_var = np.append(y_train_var, var)\n",
    "    y_train_std = np.append(y_train_std, std)\n",
    "    y_train_max = np.append(y_train_max, max)\n",
    "    y_train_min = np.append(y_train_min, min)\n",
    "\n",
    "y_train_feature = np.concatenate((y_train_mean.reshape(-1,1), y_train_var.reshape(-1,1), y_train_std.reshape(-1,1), y_train_max.reshape(-1,1), y_train_min.reshape(-1,1)), axis=1)\n",
    "y_train_feature.shape\n",
    "\n",
    "path = 'C:/Users/kbj/Downloads/data/train/z/'\n",
    "data_train_z_list = os.listdir('C:/Users/kbj/Downloads/data/train/z/')\n",
    "path = 'C:/Users/kbj/Downloads/data/train/z/'\n",
    "z_train_mean = np.array([])\n",
    "z_train_var = np.array([])\n",
    "z_train_std = np.array([])\n",
    "z_train_max = np.array([])\n",
    "z_train_min = np.array([])\n",
    "\n",
    "for i in data_train_z_list:\n",
    "    df = pd.read_csv(path+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    z_train_mean = np.append(z_train_mean, mean)\n",
    "    z_train_var = np.append(z_train_var, var)\n",
    "    z_train_std = np.append(z_train_std, std)\n",
    "    z_train_max = np.append(z_train_max, max)\n",
    "    z_train_min = np.append(z_train_min, min)\n",
    "\n",
    "z_train_feature = np.concatenate((z_train_mean.reshape(-1,1), z_train_var.reshape(-1,1), z_train_std.reshape(-1,1), z_train_max.reshape(-1,1), z_train_min.reshape(-1,1)), axis=1)\n",
    "z_train_feature.shape\n",
    "\n",
    "path = 'C:/Users/kbj/Downloads/data/train/e/'\n",
    "data_train_e_list = os.listdir('C:/Users/kbj/Downloads/data/train/e/')\n",
    "path = 'C:/Users/kbj/Downloads/data/train/e/'\n",
    "e_train_mean = np.array([])\n",
    "e_train_var = np.array([])\n",
    "e_train_std = np.array([])\n",
    "e_train_max = np.array([])\n",
    "e_train_min = np.array([])\n",
    "\n",
    "for i in data_train_e_list:\n",
    "    df = pd.read_csv(path+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    e_train_mean = np.append(e_train_mean, mean)\n",
    "    e_train_var = np.append(e_train_var, var)\n",
    "    e_train_std = np.append(e_train_std, std)\n",
    "    e_train_max = np.append(e_train_max, max)\n",
    "    e_train_min = np.append(e_train_min, min)\n",
    "\n",
    "e_train_feature = np.concatenate((e_train_mean.reshape(-1,1), e_train_var.reshape(-1,1), e_train_std.reshape(-1,1), e_train_max.reshape(-1,1), e_train_min.reshape(-1,1)), axis=1)\n",
    "e_train_feature.shape\n",
    "\n",
    "#xyz_train_feature = np.sqrt(np.square(x_train_feature[:,0])+np.square(y_train_feature[:,0])+np.square(z_train_feature[:,0]))\n",
    "#xyz_train_feature = xyz_train_feature.reshape(-1,1)\n",
    "#all_train_feature = np.concatenate((x_train_feature, y_train_feature, z_train_feature, e_train_feature, xyz_train_feature), axis=1)\n",
    "all_train_feature = np.concatenate((x_train_feature, y_train_feature, z_train_feature, e_train_feature), axis=1)\n",
    "all_train_feature.shape\n",
    "\n",
    "train_label = pd.read_csv('C:/Users/kbj/Downloads/data/train/train_label.csv', header=None)\n",
    "train_label = train_label.to_numpy()\n",
    "\n",
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 5)\n",
      "(1400, 5)\n",
      "(1400, 5)\n",
      "(1400, 5)\n",
      "(1400, 20)\n",
      "(1400, 1)\n"
     ]
    }
   ],
   "source": [
    "path_x = 'C:/Users/kbj/Downloads/data/test/x/'\n",
    "data_test_x_list = os.listdir('C:/Users/kbj/Downloads/data/test/x/')\n",
    "x_test_mean = np.array([])\n",
    "x_test_var = np.array([])\n",
    "x_test_std = np.array([])\n",
    "x_test_max = np.array([])\n",
    "x_test_min = np.array([])\n",
    "\n",
    "for i in data_test_x_list:\n",
    "    df = pd.read_csv(path_x+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    x_test_mean = np.append(x_test_mean, mean)\n",
    "    x_test_var = np.append(x_test_var, var)\n",
    "    x_test_std = np.append(x_test_std, std)\n",
    "    x_test_max = np.append(x_test_max, max)\n",
    "    x_test_min = np.append(x_test_min, min)\n",
    "\n",
    "x_test_feature = np.concatenate((x_test_mean.reshape(-1,1), x_test_var.reshape(-1,1), x_test_std.reshape(-1,1), x_test_max.reshape(-1,1), x_test_min.reshape(-1,1)), axis=1)\n",
    "print(x_test_feature.shape)\n",
    "\n",
    "path_y = 'C:/Users/kbj/Downloads/data/test/y/'\n",
    "data_test_y_list = os.listdir('C:/Users/kbj/Downloads/data/test/y/')\n",
    "y_test_mean = np.array([])\n",
    "y_test_var = np.array([])\n",
    "y_test_std = np.array([])\n",
    "y_test_max = np.array([])\n",
    "y_test_min = np.array([])\n",
    "\n",
    "for i in data_test_y_list:\n",
    "    df = pd.read_csv(path_y+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    y_test_mean = np.append(y_test_mean, mean)\n",
    "    y_test_var = np.append(y_test_var, var)\n",
    "    y_test_std = np.append(y_test_std, std)\n",
    "    y_test_max = np.append(y_test_max, max)\n",
    "    y_test_min = np.append(y_test_min, min)\n",
    "\n",
    "y_test_feature = np.concatenate((y_test_mean.reshape(-1,1), y_test_var.reshape(-1,1), y_test_std.reshape(-1,1), y_test_max.reshape(-1,1), y_test_min.reshape(-1,1)), axis=1)\n",
    "print(y_test_feature.shape)\n",
    "\n",
    "path_z = 'C:/Users/kbj/Downloads/data/test/z/'\n",
    "data_test_z_list = os.listdir('C:/Users/kbj/Downloads/data/test/z/')\n",
    "z_test_mean = np.array([])\n",
    "z_test_var = np.array([])\n",
    "z_test_std = np.array([])\n",
    "z_test_max = np.array([])\n",
    "z_test_min = np.array([])\n",
    "\n",
    "for i in data_test_z_list:\n",
    "    df = pd.read_csv(path_z+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    z_test_mean = np.append(z_test_mean, mean)\n",
    "    z_test_var = np.append(z_test_var, var)\n",
    "    z_test_std = np.append(z_test_std, std)\n",
    "    z_test_max = np.append(z_test_max, max)\n",
    "    z_test_min = np.append(z_test_min, min)\n",
    "\n",
    "z_test_feature = np.concatenate((z_test_mean.reshape(-1,1), z_test_var.reshape(-1,1), z_test_std.reshape(-1,1), z_test_max.reshape(-1,1), z_test_min.reshape(-1,1)), axis=1)\n",
    "print(z_test_feature.shape)\n",
    "\n",
    "path_e = 'C:/Users/kbj/Downloads/data/test/e/'\n",
    "data_test_e_list = os.listdir('C:/Users/kbj/Downloads/data/test/e/')\n",
    "e_test_mean = np.array([])\n",
    "e_test_var = np.array([])\n",
    "e_test_std = np.array([])\n",
    "e_test_max = np.array([])\n",
    "e_test_min = np.array([])\n",
    "\n",
    "for i in data_test_e_list:\n",
    "    df = pd.read_csv(path_e+i, header=None)\n",
    "    mean = np.mean(df.to_numpy())\n",
    "    var = np.var(df.to_numpy())\n",
    "    std = np.std(df.to_numpy())\n",
    "    max = np.max(df.to_numpy())\n",
    "    min = np.min(df.to_numpy())\n",
    "    e_test_mean = np.append(e_test_mean, mean)\n",
    "    e_test_var = np.append(e_test_var, var)\n",
    "    e_test_std = np.append(e_test_std, std)\n",
    "    e_test_max = np.append(e_test_max, max)\n",
    "    e_test_min = np.append(e_test_min, min)\n",
    "\n",
    "e_test_feature = np.concatenate((e_test_mean.reshape(-1,1), e_test_var.reshape(-1,1), e_test_std.reshape(-1,1), e_test_max.reshape(-1,1), e_test_min.reshape(-1,1)), axis=1)\n",
    "print(e_test_feature.shape)\n",
    "\n",
    "#xyz_test_feature = np.sqrt(np.square(x_test_feature[:,0])+np.square(y_test_feature[:,0])+np.square(z_test_feature[:,0]))\n",
    "#xyz_test_feature = xyz_test_feature.reshape(-1,1)\n",
    "#all_test_feature = np.concatenate((x_test_feature, y_test_feature, z_test_feature, e_test_feature, xyz_test_feature), axis=1)\n",
    "\n",
    "all_test_feature = np.concatenate((x_test_feature, y_test_feature, z_test_feature, e_test_feature), axis=1)\n",
    "print(all_test_feature.shape)\n",
    "\n",
    "test_label = pd.read_csv('C:/Users/kbj/Downloads/data/test/test_label.csv', header=None)\n",
    "test_label = test_label.to_numpy()\n",
    "\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 20)\n",
      "(700, 1)\n",
      "(1400, 20)\n",
      "(1400, 1)\n"
     ]
    }
   ],
   "source": [
    "print(all_train_feature.shape)\n",
    "print(train_label.shape)\n",
    "\n",
    "print(all_test_feature.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train = all_train_feature\n",
    "y_train = train_label.reshape(-1)\n",
    "x_test = all_test_feature\n",
    "y_test = test_label.reshape(-1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_train)\n",
    "\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(x_train_std)\n",
    "train_y = torch.LongTensor(y_train)\n",
    "test_x = torch.Tensor(x_test_std)\n",
    "test_y = torch.LongTensor(y_test)\n",
    "\n",
    "ds_train = TensorDataset(train_x, train_y)\n",
    "ds_test = TensorDataset(test_x, test_y)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=128, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'loss': 0.6821}:   1%|          | 1/100 [00:00<00:11,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for all 0 selection: tensor(4.0872e-10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.6401}: 100%|██████████| 100/100 [00:09<00:00, 11.03it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t#mnist = MNIST()\n",
    "\t#train_x, train_y, test_x, test_y = mnist.load_dataset()\n",
    "\tprint('MAE for all 0 selection:', torch.mean(train_x))\n",
    "\tvn = train_x.shape[1]\n",
    "\thn = 1600\n",
    "\n",
    "\trbm = RBM(vn, hn, epochs=100, mode='bernoulli', lr=0.0001, k=1, batch_size=64, gpu=True, optimizer='adam', savefile='phm_trained_rbm.pt', early_stopping_patience=1000)\n",
    "\trbm.train(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 200, 'loss': 1.5308, 'acc': 0.8453}: 100%|██████████| 200/200 [00:14<00:00, 13.88it/s]\n",
      "{'epoch': 200, 'loss': 1.4772, 'acc': 0.8562}: 100%|██████████| 200/200 [00:18<00:00, 10.63it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t#mnist = MNIST()\n",
    "\t#train_x, train_y, test_x, test_y = mnist.load_dataset()\n",
    "\t\n",
    "\tvn = train_x.shape[1]\n",
    "\thn = 1600\n",
    "\trbm = RBM(vn, hn)\n",
    "\trbm.load_rbm('phm_trained_rbm.pt')\n",
    "\n",
    "\tmodel = initialize_model()\n",
    "\n",
    "\tmodel, progress = train(model, train_x, train_y, train_x, train_y, test_x, test_y)\n",
    "\tprogress = pd.DataFrame(np.array(progress))\n",
    "\tprogress.columns = ['epochs', 'test loss', 'train loss', 'test acc', 'train acc']\n",
    "\tprogress.to_csv('RBM_without_pretraining_classifier.csv', index=False)\n",
    "\n",
    "\tmodel = initialize_model()\n",
    "\n",
    "\tmodel[0].weight = torch.nn.Parameter(rbm.W)\n",
    "\tmodel[0].bias = torch.nn.Parameter(rbm.hb)\n",
    "\n",
    "\tmodel, progress = train(model, train_x, train_y, train_x, train_y, test_x, test_y)\n",
    "\tprogress = pd.DataFrame(np.array(progress))\n",
    "\tprogress.columns = ['epochs', 'test loss', 'train loss', 'test acc', 'train acc']\n",
    "\tprogress.to_csv('RBM_pretrained_classifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c644af2b13e56b04db7589c61fc8993524f91b02323464e25eb4a200df9a07b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
